{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome back to Python Notebooks!\n",
    "\n",
    "Didja miss me??\n",
    "\n",
    "### And welcome to Week 4, Day 2 - introducing LangGraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "from agents import OpenAIChatCompletionsModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful constants\n",
    "\n",
    "nouns = [\"Cabbages\", \"Unicorns\", \"Toasters\", \"Penguins\", \"Bananas\", \"Zombies\", \"Rainbows\", \"Eels\", \"Pickles\", \"Muffins\"]\n",
    "adjectives = [\"outrageous\", \"smelly\", \"pedantic\", \"existential\", \"moody\", \"sparkly\", \"untrustworthy\", \"sarcastic\", \"squishy\", \"haunted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our favorite first step! Crew was doing this for us, by the way.\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shout(text: Annotated[str, \"something to be shouted\"]) -> str:\n",
    "    print(text.upper())\n",
    "    print()\n",
    "    return text.upper()\n",
    "\n",
    "shout(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shout(text: str) ->str:\n",
    "    print(text.upper())\n",
    "    return text.upper()\n",
    "\n",
    "shout(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about \"Annotated\"\n",
    "\n",
    "You probably know this; type hinting is a feature in Python that lets you specify the type of something:\n",
    "\n",
    "`my_favorite_things: List`\n",
    "\n",
    "But you may not know this:\n",
    "\n",
    "You can also use something called \"Annotated\" to add extra information that somebody else might find useful:\n",
    "\n",
    "`my_favorite_things: Annotated[List, \"these are a few of mine\"]`\n",
    "\n",
    "LangGraph needs us to use this feature when we define our State object.\n",
    "\n",
    "It wants us to tell it what function it should call to update the State with a new value.\n",
    "\n",
    "This function is called a **reducer**.\n",
    "\n",
    "LangGraph provides a default reducer called `add_messages` which takes care of the most common case.\n",
    "\n",
    "And that hopefully explains why the State looks like this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the State object\n",
    "\n",
    "You can use any python object; but it's most common to use a TypedDict or a Pydantic BaseModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class state is subclass of basemodel from pydantic, which is a particular type of dictionary in python \n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # here the add_messages are reducers that are used for indicating to langgraph what is a thing for\n",
    "    # add_messages is imported from langgraph.graph.messages\n",
    "    # ad_messages is an anotation where we can define reducers, reducer is where we define the function that will be called inorder to combine one state with another \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start the Graph Builder with this State class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "# here state is not an object, but a class, type of thing that we call our state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Node\n",
    "\n",
    "A node can be any python function.\n",
    "\n",
    "The reducer that we set before gets automatically called to combine this response with previous responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_first_node(old_state: State) -> State:\n",
    "\n",
    "    reply = f\"{random.choice(nouns)} are {random.choice(adjectives)}\"\n",
    "    messages = [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    new_state = State(messages=messages)\n",
    "\n",
    "    return new_state\n",
    "\n",
    "graph_builder.add_node(\"first_node\", our_first_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"first_node\")\n",
    "graph_builder.add_edge(\"first_node\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compile the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! Showtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str, history):\n",
    "    message = {\"role\": \"user\", \"content\": user_input}\n",
    "    messages = [message]\n",
    "    state = State(messages=messages)\n",
    "    result = graph.invoke(state)\n",
    "    print(result)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But why did I show you that?\n",
    "\n",
    "To make the point that LangGraph is all about python functions - it doesn't need to involve LLMs!!\n",
    "\n",
    "Now we'll do the 5 steps again, but in 1 shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the State object\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start the Graph Builder with this State class\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "llm = ChatOpenAI(model=gemini_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a Node\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chatbot_node(old_state: State) -> State:\n",
    "    response = llm.invoke(old_state.messages)\n",
    "    new_state = State(messages=[response])\n",
    "    return new_state\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Edges\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Graph\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! And, let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str, history):\n",
    "    initial_state = State(messages=[{\"role\": \"user\", \"content\": user_input}])\n",
    "    result = graph.invoke(initial_state)\n",
    "    print(result)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'grpc' has no attribute 'StatusCode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_messages\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m     11\u001b[39m load_dotenv(override=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m google_api_key = os.getenv(\u001b[33m'\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\langchain_google_genai\\__init__.py:60\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"**LangChain Google Generative AI Integration**\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis module integrates Google's Generative AI models, specifically the Gemini series, with the LangChain framework. It provides classes for interacting with chat models and generating embeddings, leveraging Google's advanced AI capabilities.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HarmBlockThreshold, HarmCategory, Modality\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\langchain_google_genai\\_enums.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativelanguage_v1beta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      3\u001b[39m HarmBlockThreshold = genai.SafetySetting.HarmBlockThreshold\n\u001b[32m      4\u001b[39m HarmCategory = genai.HarmCategory\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\__init__.py:21\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativelanguage_v1beta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gapic_version \u001b[38;5;28;01mas\u001b[39;00m package_version\n\u001b[32m     18\u001b[39m __version__ = package_version.__version__\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheServiceAsyncClient, CacheServiceClient\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiscuss_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiscussServiceAsyncClient, DiscussServiceClient\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileServiceAsyncClient, FileServiceClient\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\cache_service\\__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2025 Google LLC\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheServiceAsyncClient\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheServiceClient\n\u001b[32m     19\u001b[39m __all__ = (\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCacheServiceClient\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCacheServiceAsyncClient\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\cache_service\\async_client.py:32\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Callable,\n\u001b[32m     21\u001b[39m     Dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     Union,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m core_exceptions\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gapic_v1\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retry_async \u001b[38;5;28;01mas\u001b[39;00m retries\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\google\\api_core\\exceptions.py:43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrpc\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrpc_status\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rpc_status\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     45\u001b[39m     _warn_could_not_import_grpcio_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\grpc_status\\rpc_status.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrpc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m status_pb2\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrpc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRPC_DETAILS_METADATA_KEY\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m code_to_grpc_status_code\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_Status\u001b[39;00m(\n\u001b[32m     27\u001b[39m     collections.namedtuple(\u001b[33m\"\u001b[39m\u001b[33m_Status\u001b[39m\u001b[33m\"\u001b[39m, (\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdetails\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrailing_metadata\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     28\u001b[39m     grpc.Status,\n\u001b[32m     29\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DeepADabhi\\Desktop\\projects\\temp\\agents\\.venv\\Lib\\site-packages\\grpc_status\\_common.py:18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"Reference implementation for status mapping in gRPC Python.\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrpc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m _CODE_TO_GRPC_CODE_MAPPING = {x.value[\u001b[32m0\u001b[39m]: x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgrpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStatusCode\u001b[49m}\n\u001b[32m     20\u001b[39m GRPC_DETAILS_METADATA_KEY = \u001b[33m\"\u001b[39m\u001b[33mgrpc-status-details-bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcode_to_grpc_status_code\u001b[39m(code):\n",
      "\u001b[31mAttributeError\u001b[39m: module 'grpc' has no attribute 'StatusCode'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Image\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import gradio as gr\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=google_api_key)\n",
    "\n",
    "def chatbot_node(old_state: State) -> State:\n",
    "    # Invoke the LLM with the current messages\n",
    "    # sends [HumanMessage(content=\"hi\")] to the Gemini model.\n",
    "    reply = llm.invoke(old_state.messages)  # Returns AIMessage\n",
    "    \n",
    "    # Append the reply to the existing messages using add_messages\n",
    "    new_messages = old_state.messages + [reply]\n",
    "    \n",
    "    # here, add_messages ensures the HumanMessage and AIMessage are appended correctly.\n",
    "    # this returns dictionary containing \"messages\" array\n",
    "    new_state = State(messages=new_messages)\n",
    "    return new_state\n",
    "\n",
    "# Add the node and edges\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Define the chat function to handle Gradio input and history\n",
    "def chat(user_input: str, history: list):\n",
    "    # Use history directly; if empty, include the user_input\n",
    "    messages = history or [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # If history is not empty, ensure the last user input isn't duplicated\n",
    "    if history and history[-1][\"role\"] == \"user\" and history[-1][\"content\"] == user_input:\n",
    "        messages = history\n",
    "    else:\n",
    "        messages = history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "    messages = messages[-4:]\n",
    "    # Create the State object\n",
    "    # the State class processes the message using add_messages\n",
    "    # add_message converts the dictionary messages to a HumanMessage object\n",
    "    state = State(messages=messages)\n",
    "\n",
    "    # Invoke the graph\n",
    "    # this graph will run the chatbot_node\n",
    "    result = graph.invoke(state)\n",
    "    print(result)\n",
    "    # Return the latest assistant message for Gradio\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "# Launch the Gradio interface\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "# add_message is invoked when State object is created (State(messages=messages)) or updated (new_state = State(messages=new_messages))\n",
    "# add_message functionality:\n",
    "#    converts the input dictionaries to HumanMessage objects\n",
    "#    appends the AIMessage objects from LLM to the messages list\n",
    "#    it ensures the list maintains the order, (user → assistant → user → assistant)\n",
    "#--------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
