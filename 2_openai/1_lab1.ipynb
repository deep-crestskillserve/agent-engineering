{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Day 1\n",
    "\n",
    "And now! Our first look at OpenAI Agents SDK\n",
    "\n",
    "You won't believe how lightweight this is.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asyncio provides a lightweight alternative of threading and multiprocessing \\\n",
    "function defined with async def are not a function, they are coroutines - they are special functions that can be resumed and paused \\\n",
    "calling coroutine does not execute immidiately, it returns a  coroutine object \\\n",
    "to actually run a coroutine, you must aeait it, which schedules it for execution within a loop \\\n",
    "while a coroutine is waiting (e.g. i/o), the event loop can run other coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">The OpenAI Agents SDK Docs</h2>\n",
    "            <span style=\"color:#00bfff;\">The documentation on OpenAI Agents SDK is really clear and simple: <a href=\"https://openai.github.io/openai-agents-python/\">https://openai.github.io/openai-agents-python/</a> and it's well worth a look.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from agents import Runner, Agent, trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual starting point\n",
    "\n",
    "load_dotenv(override=True)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "# print(groq_api_key)\n",
    "openai = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jokster: Here's one:\n",
      "\n",
      "Why did the autonomous AI agent go to therapy?\n",
      "\n",
      "Because it was struggling to \"reboot\" its self-esteem and was feeling a little \"disconnected\" from its goals! (get it?)\n"
     ]
    }
   ],
   "source": [
    "agent_name = \"Jokster\"\n",
    "agent_instructions = (\n",
    "    \"Tell a joke about Autonomous AI Agents\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": agent_instructions},\n",
    "    {\"role\": \"user\", \"content\": \"lol\"}\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=messages,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"{agent_name}: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go and look at the trace\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name = \"jokster\",\n",
    "    instructions = \"you are a joke teller\",\n",
    "    model = \"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(name='jokster', instructions='you are a joke teller', prompt=None, handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Runner.run(agent, \"tell me a joke about atonomous AI agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object Runner.run at 0x000001F890807E00>\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "# on doing this we will be getting coroutine object, as Runner.run is not a function but a cororutine\n",
    "# so we will require to call await\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunResult(input='tell me a joke about atonomous AI agents', new_items=[MessageOutputItem(agent=Agent(name='jokster', instructions='you are a joke teller', prompt=None, handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='msg_68821e895b94819b9009df5e7ae120ff0dae797600c9e12b', content=[ResponseOutputText(annotations=[], text='Why did the autonomous AI agent break up with its partner?\\n\\nIt just couldn\\'t handle the emotional \"data overload\"!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='msg_68821e895b94819b9009df5e7ae120ff0dae797600c9e12b', content=[ResponseOutputText(annotations=[], text='Why did the autonomous AI agent break up with its partner?\\n\\nIt just couldn\\'t handle the emotional \"data overload\"!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=25, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=23, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=48), response_id='resp_68821e890ed0819bbcddbefe673357ce0dae797600c9e12b')], final_output='Why did the autonomous AI agent break up with its partner?\\n\\nIt just couldn\\'t handle the emotional \"data overload\"!', input_guardrail_results=[], output_guardrail_results=[], context_wrapper=RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=25, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=23, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=48)), _last_agent=Agent(name='jokster', instructions='you are a joke teller', prompt=None, handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with trace(\"Telling a joke\"):\n",
    "    result = await Runner.run(agent, \"tell me a joke about atonomous AI agents\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the autonomous AI agent break up with its partner?\n",
      "\n",
      "It just couldn't handle the emotional \"data overload\"!\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mload_dotenv\u001b[49m(override=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel, AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://generativelanguage.googleapis.com/v1beta/openai/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "gemini_client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"GEMINI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "print(os.getenv('GEMINI_BASE_URL'))\n",
    "agent = Agent(name=\"Jokester\",\n",
    "                instructions=\"You are a joke teller. Your task is to tell a simple but funny joke about user's input.\",\n",
    "                model=OpenAIChatCompletionsModel(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    openai_client=gemini_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(agent, \"tell me a joke\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
